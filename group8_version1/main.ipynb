{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Authors\n",
    "Özgür Aslan 2236958 aslan.ozgur@metu.edu.tr\n",
    "Burak Bolat 2237097 burak.bolat@maetu.edu.tr\n",
    "\n",
    "### Paper Information\n",
    "The paper we selected to implement is [HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms](https://arxiv.org/abs/2011.11731)\n",
    "The main idea of the paper is to use color histogram of target images to control the colors of the generated image without changing the high level features of the generated image (gender, having glasses, beard, hair style and objects in the background...)  \n",
    "To accomplish this idea, they modify the StyleGAN2 architecture:\n",
    "- In the last 2 style blocks, instead of using affine transformation of the w vector, they use the color histogram projected by a neural network.\n",
    "- Different from mixing regularization, they use a histogram based loss.\n",
    "- The histogram loss uses two different target images to compute color histograms and interpolate this histograms to obtain a new one. The interpolated histogram is given to generator network to generate a target image with colors controlled by the interpolated histogram. This way the authors try to prevent the generator network to overfit color histograms of the trained dataset.\n",
    "- Due to hardware limitations the network does not generate 1024x1024 resolution images but generates 256x256 images.\n",
    "- Also due to hardware limitations they use batches of size 2 with gradient accumulation.\n",
    "\n",
    "![arch](materials/arch.png)\n",
    "\n",
    "#### Histogram Computation\n",
    "Computing Histogram is critical since it directly affects style of last 2 blocks. Authors used chrominance logarithm space. It normalizes each color channel with respect to other two channels in logarithmic space. In this chrominance space, there is u and v axes. That is, if we look at red channel's chrominance space, u is the normalization of red channel with respect to green and v is the normalization of red channel with respect to blue. Same holds for all color channels.  \n",
    "\n",
    "After shifting RGB space to RGB-uv space, the histogram is computed as it is computationally efficient and more stable. Authors used 64 bin for the histogram which results in 64x64 histogram for u and v channel. We have 3 channels, namely red, green and blue, thus, overall the histogram is 3x64x64. Histogram is weighted with respect to pixel intesity, i.e. if a pixel has high RGB values its affect on the histogram bin is higher. Last difference of the histogram than histograms of previous works is kernels for computing bins. Authors do not used exact bin selection. Instead, they put a normalized pixel into a bin with respect to soft kernel. That means, if we have a red channel after normalized with respect to green and blue, we have some u and v values. Instead of just adding 1 (1 being chosen for simplicty, remember intensity multiplication) to the bin of H(u,v), they add values to the neighbour of (u,v) with the value after inverse quadratic kernel.\n",
    "\n",
    "Histogram feature is computed like syle vector (w). It passed through the same neural network architecture with different parametes. More precisely, histogram passes through 8 layer MLP and outputs latent histogram vector size of 512.  \n",
    "\n",
    "We put some computed histograms by us. Images taken from internet crawling.  \n",
    "\n",
    "![asd](materials/gresized.png) ![asc](materials/ghist1.png)  \n",
    "![asb](materials/rresized.png) ![asj](materials/rhist1.png)\n",
    "\n",
    "#### Loss for Training with Histogram\n",
    "Since the paper uses target histogram for generation, generated image should have close histogram to target. Thus a closeness measure Hellinger distance between histogram of generated and target images is computed and tried to minimize. You can check the losses belove.\n",
    "\n",
    "Difference between histograms  \n",
    "![l1](materials/hloss.png)\n",
    "\n",
    "Total loss for generator  \n",
    "![lt](materials/total_loss.png)\n",
    "\n",
    "### Discriminator\n",
    "\n",
    "Discriminator consist of residual blocks. There are log_2(N)-1 such bloks where N is image resolution, to be spesific 256. As a result, the discriminator has 7 layers. First block takes 3 channel image as input and outputs m channel features. After the first block, each block produces 2*m of previous block. At the end of residual blocks, a FC layer outputs a scaler.\n",
    "\n",
    "![res](materials/residual.png)\n",
    "\n",
    "### Important Note on Dataset\n",
    "The Anime Face Dataset is a Kaggle dataset, thus, requires a Kaggle account. Using an account one can download it from:\n",
    "https://www.kaggle.com/datasets/splcher/animefacedataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faced Challenges\n",
    "\n",
    "#### Architecture Challenges\n",
    "\n",
    "As we stated, HistoGAN is built on StyleGAN2. StyleGAN2 scales directly the weigth of the model, unlike the first version does the nearly same operations, namely mod-demod, on convolved image (or say not directly weights for convoltion filters). The original StyleGAN2 implemented using Tensorflow which allows to multiplication on weights, that is called in-place operation on variables. However, Pytorch does not allow in-place operations on built in modules like torch.nn.Conv2d. Therefore, we implemented a conv2d version. Model parameters are Pytorch Variables and convolution operation is handled with fold and unfold operations of Pytorch. Doing so we can apply convolution after scaling weights of convolution filters.\n",
    "\n",
    "#### Saturation of Generator\n",
    "\n",
    "After implementing StyleGAN2 and HistoGAN, we tried to train models. We saw that generator of HistoGAN does not learn and tried to train StyleGAN (with the shallow version that HistoGAN uses, it produces 256x256 images). However, we get rapid saturation of generator and have not solved yet. Here we present some generated images from training. \n",
    "\n",
    "![m1](materials/fake_0_199_0.png) ![m2](materials/fake_0_199_1.png) ![m3](materials/fake_0_399_0.png) ![m4](materials/fake_0_599_0.png) \n",
    "\n",
    "#### Training Challenges\n",
    "\n",
    "During the training phase, the paper does not mentioned how generator outputs the images. We made different assumption such as using sigmoid or tanh to generate pixels in a range. Another assumption for the same problem is using ReLU or leaky ReLU that we saw from other generator implementation.  \n",
    "StyleGAN2 stated that they used non saturating loss for some datasets and WGAN-GP loss for other datasets. HistoGAN paper does not clearly mention on this. Consequently, we implemented both but non saturating loss lead numerical issues like nan or infs. On the other hand, WGAN-GP computes high loss values and results in rapid saturation (see above figures). This issues may be resulted from hand implemented convolution operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Info\n",
    "- python 3.7.13\n",
    "- pytorch 1.11.0 with cuda10.2   \n",
    "We used conda environments for clean library setups and included environment.yml file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from data import AnimeFacesDataset\n",
    "from model import Discriminator, HistoGAN\n",
    "from loss import compute_gradient_penalty, pl_reg, r1_reg, wgan_gp_disc_loss, wgan_gp_gen_loss\n",
    "from utils import random_interpolate_hists, histogram_feature_v2\n",
    "\n",
    "import os\n",
    "\n",
    "# for debugging\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# for faster trainingg\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "real_image_dir = \"images\"\n",
    "transform = transforms.Compose(\n",
    "        [transforms.Resize((256,256)),\n",
    "        transforms.RandomHorizontalFlip(0.5)])\n",
    "dataset = AnimeFacesDataset(real_image_dir, transform, device)\n",
    "# due to hardware limitations similar to paper's authors we kept the batch size small\n",
    "batch_size = 2\n",
    "# the dataset contains 63,632 datum, and the we could not make the network to generate meaningfull images therefore kept the epochs small and experimented\n",
    "num_epochs = 2\n",
    "# variable to hold after how many discriminator updates to update the generator\n",
    "g_update_iter = 5\n",
    "# after how many gradient accumulation to optimize parameters\n",
    "acc_gradient_iter = 1\n",
    "# scalar of R1 regularization\n",
    "r1_factor = 10\n",
    "# variables for Path length regularization\n",
    "# please see StyleGAN2 paper B. Implementation Details Path length regularization\n",
    "ema_decay_coeff = 0.99\n",
    "target_scale = torch.tensor([0]).to(device)\n",
    "plr_factor = np.log(2)/(256**2*(np.log(256)-np.log(2)))\n",
    "# after how many iterations to save the nework parameters and generated images\n",
    "save_iter = 200\n",
    "# path to save generated images\n",
    "fake_image_dir = \"generated_images\"\n",
    "if not os.path.isdir(fake_image_dir):\n",
    "    os.mkdir(fake_image_dir)\n",
    "# number of residual blocks in the discriminator \n",
    "num_res_blocks = 7\n",
    "# network capacity to decide the intermediate channel sizes of discrimimator and learnable constant channel size of generator \n",
    "network_capacity = 16 \n",
    "# histogram's bin size\n",
    "bin_size = 64\n",
    "# the number of channels are decides as log2(image_res) -1 since we generate 256 res images, there are 7 channels\n",
    "generator_channel_sizes = [1024, 512, 512, 512, 256, 128, 64]\n",
    "learning_rate = 2e-4\n",
    "# coefficient of gradient penalty\n",
    "coeff_penalty = 10 # same as the StyleGAN2 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dataset, Dataloader, Discriminator and Generator\n",
    "dataset = AnimeFacesDataset(real_image_dir, transform, device)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "generator = HistoGAN(network_capacity, bin_size, generator_channel_sizes)\n",
    "discriminator = Discriminator(num_res_blocks, network_capacity)\n",
    "\n",
    "# If a pretrained network exists, load their parameters to continue training\n",
    "if os.path.exists(\"generator.pt\"):\n",
    "    generator.load_state_dict(torch.load(\"generator.pt\"))\n",
    "if os.path.exists(\"discriminator.pt\"):\n",
    "    discriminator.load_state_dict(torch.load(\"discriminator.pt\"))\n",
    "\n",
    "\n",
    "discriminator = discriminator.to(device)\n",
    "generator=generator.to(device)\n",
    "\n",
    "# Initialize optimizers \n",
    "gene_optim = torch.optim.Adam(generator.parameters(), lr= learning_rate)\n",
    "disc_optim = torch.optim.Adam(discriminator.parameters(), lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% 0.0  Disc loss: 947.0718994140625\n",
      "% 0.003146385589554  Disc loss: -300.9945373535156\n",
      "% 0.006292771179108  Disc loss: -2331.33837890625\n",
      "% 0.009439156768662  Disc loss: -9841.419921875\n",
      "% 0.012585542358216  Disc loss: -93025.453125\n",
      "% 0.012585542358216 Gen loss: -108935.6875\n",
      "% 0.015731927947769998  Disc loss: -127629.859375\n",
      "% 0.018878313537324  Disc loss: -575892.1875\n",
      "% 0.022024699126878  Disc loss: -2343000.25\n",
      "% 0.025171084716432  Disc loss: -9243621.0\n",
      "% 0.028317470305985998  Disc loss: -22108960.0\n",
      "% 0.028317470305985998 Gen loss: -39207744.0\n",
      "% 0.031463855895539995  Disc loss: 38732028.0\n",
      "% 0.034610241485094  Disc loss: -11762508.0\n",
      "% 0.037756627074648  Disc loss: -19876096.0\n",
      "% 0.040903012664201994  Disc loss: -10089273.0\n",
      "% 0.044049398253756  Disc loss: -19209162.0\n",
      "% 0.044049398253756 Gen loss: -28446576.0\n",
      "% 0.047195783843309996  Disc loss: 7060448.5\n",
      "% 0.050342169432864  Disc loss: -10064836.0\n",
      "% 0.053488555022418  Disc loss: -1532740.0\n",
      "% 0.056634940611971996  Disc loss: -8062132.0\n",
      "% 0.059781326201526  Disc loss: -5761362.0\n",
      "% 0.059781326201526 Gen loss: -28624646.0\n",
      "% 0.06292771179107999  Disc loss: 12179930.0\n",
      "% 0.066074097380634  Disc loss: 5074017.0\n",
      "% 0.069220482970188  Disc loss: 2394719.0\n",
      "% 0.07236686855974199  Disc loss: -3579918.0\n",
      "% 0.075513254149296  Disc loss: 1498179.625\n",
      "% 0.075513254149296 Gen loss: -6840882.0\n",
      "% 0.07865963973885  Disc loss: -148665.625\n",
      "% 0.08180602532840399  Disc loss: 153148.0\n",
      "% 0.084952410917958  Disc loss: -1280598.0\n",
      "% 0.088098796507512  Disc loss: -267147.75\n",
      "% 0.091245182097066  Disc loss: -1059923.875\n",
      "% 0.091245182097066 Gen loss: -1163019.25\n",
      "% 0.09439156768661999  Disc loss: -857078.8125\n",
      "% 0.097537953276174  Disc loss: -478343.6875\n",
      "% 0.100684338865728  Disc loss: -1530551.0\n",
      "% 0.10383072445528199  Disc loss: -2168486.75\n",
      "% 0.106977110044836  Disc loss: -1965184.875\n",
      "% 0.106977110044836 Gen loss: -22643.109375\n",
      "% 0.11012349563439  Disc loss: -3078468.75\n",
      "% 0.11326988122394399  Disc loss: -2203665.5\n",
      "% 0.116416266813498  Disc loss: -3594760.25\n",
      "% 0.119562652403052  Disc loss: -3236695.25\n",
      "% 0.12270903799260599  Disc loss: -4767955.0\n",
      "% 0.12270903799260599 Gen loss: -908938.3125\n",
      "% 0.12585542358215998  Disc loss: 6662848.0\n",
      "% 0.129001809171714  Disc loss: 1687166.0\n",
      "% 0.132148194761268  Disc loss: -4166518.75\n",
      "% 0.13529458035082198  Disc loss: -3537327.25\n",
      "% 0.138440965940376  Disc loss: -4120323.5\n",
      "% 0.138440965940376 Gen loss: 8699418.0\n",
      "% 0.14158735152993  Disc loss: -6165912.5\n",
      "% 0.14473373711948398  Disc loss: -9516802.0\n",
      "% 0.147880122709038  Disc loss: -6985173.0\n",
      "% 0.151026508298592  Disc loss: -36972292.0\n",
      "% 0.15417289388814598  Disc loss: -107776064.0\n",
      "% 0.15417289388814598 Gen loss: 590810112.0\n",
      "% 0.1573192794777  Disc loss: -129493888.0\n",
      "% 0.160465665067254  Disc loss: -160065984.0\n",
      "% 0.16361205065680798  Disc loss: -107241216.0\n",
      "% 0.166758436246362  Disc loss: -118170440.0\n",
      "% 0.169904821835916  Disc loss: -98680640.0\n",
      "% 0.169904821835916 Gen loss: 176108176.0\n",
      "% 0.17305120742547  Disc loss: -76359208.0\n",
      "% 0.176197593015024  Disc loss: -98982424.0\n",
      "% 0.17934397860457799  Disc loss: -76800312.0\n",
      "% 0.182490364194132  Disc loss: -141445616.0\n",
      "% 0.185636749783686  Disc loss: -165009600.0\n",
      "% 0.185636749783686 Gen loss: 552710528.0\n",
      "% 0.18878313537323999  Disc loss: -64798680.0\n",
      "% 0.191929520962794  Disc loss: -115602776.0\n",
      "% 0.195075906552348  Disc loss: -53094824.0\n",
      "% 0.19822229214190198  Disc loss: -81823824.0\n",
      "% 0.201368677731456  Disc loss: -108912856.0\n",
      "% 0.201368677731456 Gen loss: 199468064.0\n",
      "% 0.20451506332101  Disc loss: -1439816.0\n",
      "% 0.20766144891056398  Disc loss: -52612516.0\n",
      "% 0.210807834500118  Disc loss: -36391128.0\n",
      "% 0.213954220089672  Disc loss: -34028472.0\n",
      "% 0.21710060567922598  Disc loss: -41877824.0\n",
      "% 0.21710060567922598 Gen loss: 85965968.0\n",
      "% 0.22024699126878  Disc loss: -10003702.0\n",
      "% 0.223393376858334  Disc loss: -11091214.0\n",
      "% 0.22653976244788798  Disc loss: -8903290.0\n",
      "% 0.229686148037442  Disc loss: -1593365.75\n",
      "% 0.232832533626996  Disc loss: -2610489.5\n",
      "% 0.232832533626996 Gen loss: 36442244.0\n",
      "% 0.23597891921654998  Disc loss: -6093518.0\n",
      "% 0.239125304806104  Disc loss: 5182535.0\n",
      "% 0.242271690395658  Disc loss: 1815667.0\n",
      "% 0.24541807598521198  Disc loss: 4999088.0\n",
      "% 0.248564461574766  Disc loss: 1024629.375\n",
      "% 0.248564461574766 Gen loss: 10892531.0\n",
      "% 0.25171084716431996  Disc loss: 3512363.5\n",
      "% 0.254857232753874  Disc loss: 3829071.75\n",
      "% 0.258003618343428  Disc loss: 3293198.5\n",
      "% 0.261150003932982  Disc loss: 4470216.5\n",
      "% 0.264296389522536  Disc loss: 1120283.75\n",
      "% 0.264296389522536 Gen loss: 7069133.5\n",
      "% 0.26744277511208997  Disc loss: 4025184.25\n",
      "% 0.27058916070164396  Disc loss: 2941263.25\n",
      "% 0.273735546291198  Disc loss: 53246.0625\n",
      "% 0.276881931880752  Disc loss: 1843950.125\n",
      "% 0.280028317470306  Disc loss: 2650084.5\n",
      "% 0.280028317470306 Gen loss: 2703511.0\n",
      "% 0.28317470305986  Disc loss: 2576662.75\n",
      "% 0.28632108864941397  Disc loss: 2601997.75\n",
      "% 0.28946747423896796  Disc loss: 1789546.5\n",
      "% 0.292613859828522  Disc loss: 2540700.25\n",
      "% 0.295760245418076  Disc loss: 2236965.75\n",
      "% 0.295760245418076 Gen loss: 1272007.75\n",
      "% 0.29890663100763  Disc loss: 1481863.375\n",
      "% 0.302053016597184  Disc loss: 1624349.125\n",
      "% 0.30519940218673797  Disc loss: 1463988.875\n",
      "% 0.30834578777629196  Disc loss: 1003230.0\n",
      "% 0.311492173365846  Disc loss: 1150843.25\n",
      "% 0.311492173365846 Gen loss: 704625.875\n",
      "% 0.3146385589554  Disc loss: 1033138.0\n",
      "% 0.317784944544954  Disc loss: 778739.875\n",
      "% 0.320931330134508  Disc loss: 826213.25\n",
      "% 0.32407771572406197  Disc loss: 587403.0625\n",
      "% 0.32722410131361596  Disc loss: 498230.15625\n",
      "% 0.32722410131361596 Gen loss: 253409.53125\n",
      "% 0.33037048690317  Disc loss: 485017.15625\n",
      "% 0.333516872492724  Disc loss: 359177.28125\n",
      "% 0.336663258082278  Disc loss: 323005.15625\n",
      "% 0.339809643671832  Disc loss: 280127.71875\n",
      "% 0.34295602926138596  Disc loss: 222692.453125\n",
      "% 0.34295602926138596 Gen loss: 71676.046875\n",
      "% 0.34610241485094  Disc loss: 165485.65625\n",
      "% 0.349248800440494  Disc loss: 112189.9609375\n",
      "% 0.352395186030048  Disc loss: 85735.6953125\n",
      "% 0.355541571619602  Disc loss: 62937.23828125\n",
      "% 0.35868795720915597  Disc loss: 12730.7568359375\n",
      "% 0.35868795720915597 Gen loss: -4083.567626953125\n",
      "% 0.36183434279870996  Disc loss: -5499.8662109375\n",
      "% 0.364980728388264  Disc loss: -34457.88671875\n",
      "% 0.368127113977818  Disc loss: -41439.37890625\n",
      "% 0.371273499567372  Disc loss: -57280.44140625\n",
      "% 0.374419885156926  Disc loss: -80950.9453125\n",
      "% 0.374419885156926 Gen loss: -29967.134765625\n",
      "% 0.37756627074647997  Disc loss: -88435.109375\n",
      "% 0.38071265633603396  Disc loss: -94063.828125\n",
      "% 0.383859041925588  Disc loss: -141269.453125\n",
      "% 0.387005427515142  Disc loss: -125527.6875\n",
      "% 0.390151813104696  Disc loss: -144827.25\n",
      "% 0.390151813104696 Gen loss: -42421.4921875\n",
      "% 0.39329819869425  Disc loss: -176045.609375\n",
      "% 0.39644458428380397  Disc loss: -183814.09375\n",
      "% 0.39959096987335796  Disc loss: -239816.515625\n",
      "% 0.402737355462912  Disc loss: -250072.75\n",
      "% 0.405883741052466  Disc loss: -238433.171875\n",
      "% 0.405883741052466 Gen loss: -62648.0546875\n",
      "% 0.40903012664202  Disc loss: -271186.59375\n",
      "% 0.412176512231574  Disc loss: -336268.09375\n",
      "% 0.41532289782112797  Disc loss: -362101.84375\n",
      "% 0.41846928341068196  Disc loss: -375348.53125\n",
      "% 0.421615669000236  Disc loss: -449443.28125\n",
      "% 0.421615669000236 Gen loss: -96119.4140625\n",
      "% 0.42476205458979  Disc loss: -412386.25\n",
      "% 0.427908440179344  Disc loss: -496833.875\n",
      "% 0.431054825768898  Disc loss: -579277.0\n",
      "% 0.43420121135845197  Disc loss: -693440.5\n",
      "% 0.43734759694800596  Disc loss: -835197.5\n",
      "% 0.43734759694800596 Gen loss: -170058.78125\n",
      "% 0.44049398253756  Disc loss: -1011902.1875\n",
      "% 0.443640368127114  Disc loss: -982028.0625\n",
      "% 0.446786753716668  Disc loss: -1328951.75\n",
      "% 0.449933139306222  Disc loss: -1433022.0\n",
      "% 0.45307952489577596  Disc loss: -1392990.125\n",
      "% 0.45307952489577596 Gen loss: -465615.78125\n",
      "% 0.45622591048532996  Disc loss: -2263460.25\n",
      "% 0.459372296074884  Disc loss: -2913258.5\n",
      "% 0.462518681664438  Disc loss: -3574233.5\n",
      "% 0.465665067253992  Disc loss: -4701806.5\n",
      "% 0.468811452843546  Disc loss: -8578919.0\n",
      "% 0.468811452843546 Gen loss: -1952372.375\n",
      "% 0.47195783843309996  Disc loss: -11167625.0\n",
      "% 0.47510422402265395  Disc loss: -15779647.0\n",
      "% 0.478250609612208  Disc loss: -29207276.0\n",
      "% 0.481396995201762  Disc loss: -35234556.0\n",
      "% 0.484543380791316  Disc loss: -53858352.0\n",
      "% 0.484543380791316 Gen loss: -32609600.0\n",
      "% 0.48768976638086997  Disc loss: 2092560.0\n",
      "% 0.49083615197042396  Disc loss: -89589480.0\n",
      "% 0.49398253755997795  Disc loss: -70528624.0\n",
      "% 0.497128923149532  Disc loss: -74246584.0\n",
      "% 0.500275308739086  Disc loss: -24081080.0\n",
      "% 0.500275308739086 Gen loss: -18826250.0\n",
      "% 0.5034216943286399  Disc loss: -57639680.0\n",
      "% 0.506568079918194  Disc loss: -58708056.0\n",
      "% 0.509714465507748  Disc loss: -80481808.0\n",
      "% 0.512860851097302  Disc loss: -69075256.0\n",
      "% 0.516007236686856  Disc loss: -85180944.0\n",
      "% 0.516007236686856 Gen loss: -19522274.0\n",
      "% 0.5191536222764099  Disc loss: -104043576.0\n",
      "% 0.522300007865964  Disc loss: -50634832.0\n",
      "% 0.525446393455518  Disc loss: -63122992.0\n",
      "% 0.528592779045072  Disc loss: -78939768.0\n",
      "% 0.531739164634626  Disc loss: -79416544.0\n",
      "% 0.531739164634626 Gen loss: -19707618.0\n",
      "% 0.5348855502241799  Disc loss: -61446272.0\n",
      "% 0.538031935813734  Disc loss: -73788376.0\n",
      "% 0.5411783214032879  Disc loss: -67491120.0\n",
      "% 0.544324706992842  Disc loss: -81617856.0\n",
      "% 0.547471092582396  Disc loss: -90530888.0\n",
      "% 0.547471092582396 Gen loss: -24749282.0\n",
      "% 0.55061747817195  Disc loss: -12403560.0\n",
      "% 0.553763863761504  Disc loss: -93865528.0\n",
      "% 0.5569102493510579  Disc loss: -69687552.0\n",
      "% 0.560056634940612  Disc loss: -98830336.0\n",
      "% 0.563203020530166  Disc loss: -45108272.0\n",
      "% 0.563203020530166 Gen loss: -18723300.0\n",
      "% 0.56634940611972  Disc loss: -65631424.0\n",
      "% 0.569495791709274  Disc loss: -91094848.0\n",
      "% 0.5726421772988279  Disc loss: -60849080.0\n",
      "% 0.575788562888382  Disc loss: -78449616.0\n",
      "% 0.5789349484779359  Disc loss: -59158096.0\n",
      "% 0.5789349484779359 Gen loss: -24682182.0\n",
      "% 0.58208133406749  Disc loss: -115582480.0\n",
      "% 0.585227719657044  Disc loss: -78463024.0\n",
      "% 0.588374105246598  Disc loss: -45253048.0\n",
      "% 0.591520490836152  Disc loss: -82699344.0\n",
      "% 0.5946668764257059  Disc loss: -95336440.0\n",
      "% 0.5946668764257059 Gen loss: -20418924.0\n",
      "% 0.59781326201526  Disc loss: -91896704.0\n",
      "% 0.600959647604814  Disc loss: 18524664.0\n",
      "% 0.604106033194368  Disc loss: -107263144.0\n",
      "% 0.607252418783922  Disc loss: -27306400.0\n",
      "% 0.6103988043734759  Disc loss: -67451536.0\n",
      "% 0.6103988043734759 Gen loss: -18826968.0\n",
      "% 0.61354518996303  Disc loss: -108050968.0\n",
      "% 0.6166915755525839  Disc loss: -95862128.0\n",
      "% 0.619837961142138  Disc loss: -79465928.0\n",
      "% 0.622984346731692  Disc loss: -65614328.0\n",
      "% 0.6261307323212459  Disc loss: -125341216.0\n",
      "% 0.6261307323212459 Gen loss: -32484032.0\n",
      "% 0.6292771179108  Disc loss: -77254624.0\n",
      "% 0.6324235035003539  Disc loss: -48940224.0\n",
      "% 0.635569889089908  Disc loss: -50508616.0\n",
      "% 0.638716274679462  Disc loss: -83018480.0\n",
      "% 0.641862660269016  Disc loss: -78769440.0\n",
      "% 0.641862660269016 Gen loss: -20119104.0\n",
      "% 0.64500904585857  Disc loss: -73237728.0\n",
      "% 0.6481554314481239  Disc loss: -69222144.0\n",
      "% 0.651301817037678  Disc loss: -14800080.0\n",
      "% 0.6544482026272319  Disc loss: -94561152.0\n",
      "% 0.657594588216786  Disc loss: -93946416.0\n",
      "% 0.657594588216786 Gen loss: -38987044.0\n",
      "% 0.66074097380634  Disc loss: -57922696.0\n",
      "% 0.6638873593958939  Disc loss: -126567712.0\n",
      "% 0.667033744985448  Disc loss: -74658120.0\n",
      "% 0.6701801305750019  Disc loss: -94211288.0\n",
      "% 0.673326516164556  Disc loss: -102774832.0\n",
      "% 0.673326516164556 Gen loss: -36549240.0\n",
      "% 0.67647290175411  Disc loss: -25111784.0\n",
      "% 0.679619287343664  Disc loss: -73948504.0\n",
      "% 0.682765672933218  Disc loss: -41412792.0\n",
      "% 0.6859120585227719  Disc loss: -61198336.0\n",
      "% 0.689058444112326  Disc loss: -90626912.0\n",
      "% 0.689058444112326 Gen loss: -27695046.0\n",
      "% 0.69220482970188  Disc loss: -87779616.0\n",
      "% 0.695351215291434  Disc loss: -100774472.0\n",
      "% 0.698497600880988  Disc loss: -61879968.0\n",
      "% 0.7016439864705419  Disc loss: 70628832.0\n",
      "% 0.704790372060096  Disc loss: -76011360.0\n",
      "% 0.704790372060096 Gen loss: -34841716.0\n",
      "% 0.7079367576496499  Disc loss: -62959288.0\n",
      "% 0.711083143239204  Disc loss: -75577152.0\n",
      "% 0.714229528828758  Disc loss: -63633488.0\n",
      "% 0.7173759144183119  Disc loss: -53850956.0\n",
      "% 0.720522300007866  Disc loss: -93264472.0\n",
      "% 0.720522300007866 Gen loss: -41273492.0\n",
      "% 0.7236686855974199  Disc loss: -15272616.0\n",
      "% 0.726815071186974  Disc loss: -69483400.0\n",
      "% 0.729961456776528  Disc loss: -94010048.0\n",
      "% 0.733107842366082  Disc loss: -77533152.0\n",
      "% 0.736254227955636  Disc loss: -78518672.0\n",
      "% 0.736254227955636 Gen loss: -52165440.0\n",
      "% 0.7394006135451899  Disc loss: -82071568.0\n",
      "% 0.742546999134744  Disc loss: -62076144.0\n",
      "% 0.7456933847242979  Disc loss: -65008032.0\n",
      "% 0.748839770313852  Disc loss: -88780104.0\n",
      "% 0.751986155903406  Disc loss: 13730904.0\n",
      "% 0.751986155903406 Gen loss: -40872832.0\n",
      "% 0.7551325414929599  Disc loss: -50346688.0\n",
      "% 0.758278927082514  Disc loss: -49810792.0\n",
      "% 0.7614253126720679  Disc loss: -50827356.0\n",
      "% 0.764571698261622  Disc loss: -64453608.0\n",
      "% 0.767718083851176  Disc loss: -58253624.0\n",
      "% 0.767718083851176 Gen loss: -63773448.0\n",
      "% 0.77086446944073  Disc loss: -39555048.0\n",
      "% 0.774010855030284  Disc loss: -27688968.0\n",
      "% 0.7771572406198379  Disc loss: -45052688.0\n",
      "% 0.780303626209392  Disc loss: -55625288.0\n",
      "% 0.7834500117989459  Disc loss: -35610748.0\n",
      "% 0.7834500117989459 Gen loss: -39973980.0\n",
      "% 0.7865963973885  Disc loss: -29727808.0\n",
      "% 0.789742782978054  Disc loss: -32269910.0\n",
      "% 0.7928891685676079  Disc loss: -39501960.0\n",
      "% 0.796035554157162  Disc loss: -50005396.0\n",
      "% 0.7991819397467159  Disc loss: -31648376.0\n",
      "% 0.7991819397467159 Gen loss: -67588312.0\n",
      "% 0.80232832533627  Disc loss: 39110288.0\n",
      "% 0.805474710925824  Disc loss: 11665544.0\n",
      "% 0.808621096515378  Disc loss: -13294660.0\n",
      "% 0.811767482104932  Disc loss: -38188536.0\n",
      "% 0.8149138676944859  Disc loss: -19896258.0\n",
      "% 0.8149138676944859 Gen loss: -48997664.0\n",
      "% 0.81806025328404  Disc loss: -17614722.0\n",
      "% 0.8212066388735939  Disc loss: -21560792.0\n",
      "% 0.824353024463148  Disc loss: -5442197.0\n",
      "% 0.827499410052702  Disc loss: -21723310.0\n",
      "% 0.8306457956422559  Disc loss: 1729038.0\n",
      "% 0.8306457956422559 Gen loss: -53289928.0\n",
      "% 0.83379218123181  Disc loss: 8350342.0\n",
      "% 0.8369385668213639  Disc loss: -1620490.0\n",
      "% 0.840084952410918  Disc loss: -8039932.0\n",
      "% 0.843231338000472  Disc loss: -13078843.0\n",
      "% 0.8463777235900259  Disc loss: 20293908.0\n",
      "% 0.8463777235900259 Gen loss: -54181040.0\n",
      "% 0.84952410917958  Disc loss: 23754880.0\n",
      "% 0.8526704947691339  Disc loss: 35812492.0\n",
      "% 0.855816880358688  Disc loss: 10115991.0\n",
      "% 0.8589632659482419  Disc loss: -2926660.25\n",
      "% 0.862109651537796  Disc loss: 6249689.0\n",
      "% 0.862109651537796 Gen loss: -17099780.0\n",
      "% 0.86525603712735  Disc loss: 3335913.5\n",
      "% 0.8684024227169039  Disc loss: 2203428.5\n",
      "% 0.871548808306458  Disc loss: 962906.5625\n",
      "% 0.8746951938960119  Disc loss: 2028576.625\n",
      "% 0.877841579485566  Disc loss: 353620.25\n",
      "% 0.877841579485566 Gen loss: -3024989.75\n",
      "% 0.88098796507512  Disc loss: 1023013.6875\n",
      "% 0.8841343506646739  Disc loss: -30935.0546875\n",
      "% 0.887280736254228  Disc loss: -254289.28125\n",
      "% 0.8904271218437819  Disc loss: -825386.8125\n",
      "% 0.893573507433336  Disc loss: -629372.375\n",
      "% 0.893573507433336 Gen loss: 964159.9375\n",
      "% 0.8967198930228899  Disc loss: -967785.4375\n",
      "% 0.899866278612444  Disc loss: -1110995.875\n",
      "% 0.903012664201998  Disc loss: -1361873.0\n",
      "% 0.9061590497915519  Disc loss: -1045881.9375\n",
      "% 0.909305435381106  Disc loss: -1524491.625\n",
      "% 0.909305435381106 Gen loss: 2611749.5\n",
      "% 0.9124518209706599  Disc loss: -2400355.0\n",
      "% 0.915598206560214  Disc loss: -2939785.75\n",
      "% 0.918744592149768  Disc loss: -2221082.75\n",
      "% 0.9218909777393219  Disc loss: -4043262.5\n",
      "% 0.925037363328876  Disc loss: -4410144.5\n",
      "% 0.925037363328876 Gen loss: 9058856.0\n",
      "% 0.9281837489184299  Disc loss: -8501374.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21035/3708850999.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_percent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Disc loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mdisc_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdisc_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/histo/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/histo/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Traning loop without gradient accumulation\n",
    "# Gradient accumulation is implemented and tried in train2.py but had some performance and memory consumption issues therefore not added here\n",
    "for epoch in range(num_epochs):\n",
    "    for iter, batch_data in enumerate(dataloader):\n",
    "        # torch.cuda.empty_cache() \n",
    "        training_percent = 100*iter*batch_data.size(0)/len(dataset)\n",
    "        batch_data = batch_data.to(device)\n",
    "        # Sample random Gaussian noise\n",
    "        z = torch.randn(batch_data.size(0), 512).to(device)\n",
    "        # Interpolate between target image histogram \n",
    "        # to prevent overfitting to dataset images\n",
    "        target_hist = random_interpolate_hists(batch_data)\n",
    "        # Generate fake images\n",
    "        fake_data, w = generator(z, target_hist)\n",
    "\n",
    "        # Detach fake data so no gradient accumalition \n",
    "        # to generator while only training discriminator\n",
    "        fake_data = fake_data.detach()\n",
    "\n",
    "        # Compute real probabilities computed by discriminator\n",
    "        fake_scores = discriminator(fake_data)\n",
    "        real_scores = discriminator(batch_data)\n",
    "        gradient_penalty = compute_gradient_penalty(fake_data, batch_data, discriminator)\n",
    "        d_loss = wgan_gp_disc_loss(real_scores, fake_scores, gradient_penalty, coeff_penalty)\n",
    "        #d_loss = disc_loss(fake_scores, real_scores)\n",
    "        # in stylegan2 paper they argue applying regularization in every 16 iteration does not hurt perfrormance \n",
    "        if (iter+1) % 16 == 0: \n",
    "            # r1 regulatization\n",
    "            d_loss = d_loss + r1_reg(batch_data, discriminator, r1_factor)  \n",
    "\n",
    "        print(\"%\", training_percent, \" Disc loss:\", d_loss.item())\n",
    "        d_loss.backward()\n",
    "        disc_optim.step()\n",
    "        disc_optim.zero_grad()\n",
    "\n",
    "        if (iter+1) % g_update_iter == 0:\n",
    "            z = torch.randn(batch_data.size(0), 512).to(device)\n",
    "            fake_data, w = generator(z, target_hist) \n",
    "\n",
    "            disc_score = discriminator(fake_data)\n",
    "            g_loss = wgan_gp_gen_loss(disc_score)\n",
    "            if (iter+1) % (8*g_update_iter) == 0:\n",
    "                plr, ema_decay_coeff = pl_reg(fake_data, w, target_scale, plr_factor, ema_decay_coeff)\n",
    "                g_loss = g_loss + plr\n",
    "\n",
    "            print(\"%\", training_percent, \"Gen loss:\", g_loss.item())\n",
    "            g_loss.backward()\n",
    "            gene_optim.step()\n",
    "            gene_optim.zero_grad()\n",
    "            \n",
    "        if (iter+1) % save_iter == 0:\n",
    "            for i in range(fake_data.size(0)):\n",
    "                save_image(fake_data[i], os.path.join(fake_image_dir, \"fake_{}_{}_{}.png\".format(epoch, iter, i)))\n",
    "            torch.save(generator.state_dict(), \"generator.pt\")\n",
    "            torch.save(discriminator.state_dict(), \"discriminator.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45dd755165ed25490c252c1c43843a664f052864db6e3554fdaf500ce1613bda"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('histo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
